{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ta8mp4Lo1B01"
   },
   "outputs": [],
   "source": [
    "#******************************************************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4VRdgqW704MQ"
   },
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('/content/chatgpt_review_dataset.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HNfd0n3u03-h"
   },
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oJnwa29203up"
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0h5Q3Tjl03dW"
   },
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q0pbmVGQqSqh"
   },
   "outputs": [],
   "source": [
    "# Convert rating to sentiment\n",
    "def rating_to_sentiment(rating):\n",
    "    if rating >= 4:\n",
    "        return 'Positive'\n",
    "    elif rating == 3:\n",
    "        return 'Neutral'\n",
    "    else:\n",
    "        return 'Negative'\n",
    "\n",
    "df['sentiment'] = df['rating'].apply(rating_to_sentiment)\n",
    "df['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q5ldFs7Y1YE-"
   },
   "outputs": [],
   "source": [
    "# Plotting Sentiment Distribution\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.countplot(x='sentiment', hue='sentiment',data=df, palette='magma')\n",
    "plt.title('Sentiment Distribution')\n",
    "plt.xlabel('Sentiment')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UqjWHRWr1YAT"
   },
   "outputs": [],
   "source": [
    "# Percentage distribution\n",
    "sentiment_counts = df['sentiment'].value_counts(normalize=True) * 100\n",
    "print(sentiment_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OpSTvWwS1X7d"
   },
   "outputs": [],
   "source": [
    "# Word cloud for each sentiment\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "for label in df['sentiment'].unique():\n",
    "    text = ' '.join(df[df['sentiment'] == label]['cleaned_text'])\n",
    "    wc = WordCloud(width=800, height=400, background_color='white',\n",
    "                   colormap='viridis', max_words=100).generate(text)\n",
    "    plt.figure(figsize=(8,5))\n",
    "    plt.imshow(wc, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.title(f'Most Common Words in {label} Reviews')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B2SwHUmQ1X1s"
   },
   "outputs": [],
   "source": [
    "# Histogram for most frequent word\n",
    "\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "\n",
    "# Combine all cleaned text\n",
    "all_words = ' '.join(df['cleaned_text']).split()\n",
    "\n",
    "# Count most common 20 words\n",
    "common_words = Counter(all_words).most_common(20)\n",
    "words_df = pd.DataFrame(common_words, columns=['word', 'count'])\n",
    "\n",
    "# Plot histogram\n",
    "plt.figure(figsize=(8,5))\n",
    "sns.barplot(x='count',hue='count', y='word', data=words_df, palette='deep')\n",
    "plt.title('Top 20 Most Frequent Words in Reviews')\n",
    "plt.xlabel('Frequency')\n",
    "plt.ylabel('Word')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xHsDIwSr1Xyz"
   },
   "outputs": [],
   "source": [
    "#Rating Distribution\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(7,4))\n",
    "sns.countplot(x='rating', hue='rating', data=df, palette='coolwarm', edgecolor='black')\n",
    "\n",
    "plt.title(\"Rating Distribution\", fontsize=14, weight='bold')\n",
    "plt.xlabel(\"User Rating\")\n",
    "plt.ylabel(\"Number of Reviews\")\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.5)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iVFkxHzZ1Xuj"
   },
   "outputs": [],
   "source": [
    "# Average Rating by platform\n",
    "plt.figure(figsize=(7,4))\n",
    "sns.barplot(x='platform',hue='platform', y='rating', data=df, palette='magma', estimator='mean', errorbar=None)\n",
    "\n",
    "plt.title(\"Average Rating by Platform\", fontsize=14, weight='bold')\n",
    "plt.xlabel(\"Platform\")\n",
    "plt.ylabel(\"Average Rating\")\n",
    "plt.ylim(0, 5)  # since ratings are usually 1–5\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.5)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r_OU4oH9uP0c"
   },
   "outputs": [],
   "source": [
    "#Average Rating by Verified Status\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.barplot(x='verified_purchase',hue='verified_purchase', y='rating', data=df,\n",
    "            palette='coolwarm', estimator='mean', errorbar=None)\n",
    "\n",
    "plt.title(\"Average Rating: Verified vs Non-Verified Users\", fontsize=14, weight='bold')\n",
    "plt.xlabel(\"Verified Purchase Status\")\n",
    "plt.ylabel(\"Average Rating\")\n",
    "plt.ylim(0, 5)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.4)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VqWCHdZE1quJ"
   },
   "outputs": [],
   "source": [
    "#Review length by rating\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Group by rating and calculate average review length\n",
    "avg_length_by_rating = df.groupby('rating')['review_length'].mean()\n",
    "\n",
    "# Plot bar chart\n",
    "plt.figure(figsize=(8,5))\n",
    "avg_length_by_rating.plot(kind='bar', color='skyblue')\n",
    "plt.title(\"Average Review Length by Rating\")\n",
    "plt.xlabel(\"Rating\")\n",
    "plt.ylabel(\"Average Review Length (words)\")\n",
    "plt.xticks(rotation=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H9BbTEsu1qp2"
   },
   "outputs": [],
   "source": [
    "# Sentiment-wise Word Frequency Visualization\n",
    "\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Loop through each sentiment category\n",
    "for label in df['sentiment'].unique():\n",
    "    # Combine all words for this sentiment\n",
    "    text = ' '.join(df[df['sentiment'] == label]['cleaned_text']).split()\n",
    "\n",
    "    # Get top 15 most frequent words\n",
    "    common_words = Counter(text).most_common(15)\n",
    "    words_df = pd.DataFrame(common_words, columns=['word', 'count'])\n",
    "\n",
    "    # Plot histogram\n",
    "    plt.figure(figsize=(8,5))\n",
    "    sns.barplot(x='count',hue='count', y='word', data=words_df, palette='plasma')\n",
    "    plt.title(f'Top Words in {label} Reviews')\n",
    "    plt.xlabel('Frequency')\n",
    "    plt.ylabel('Word')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Mqlu3eNZ1qmd"
   },
   "outputs": [],
   "source": [
    "#Average Rating over time\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Ensure 'date' column is datetime\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "# Group by date and calculate average rating\n",
    "avg_rating = df.groupby(df['date'].dt.to_period('M'))['rating'].mean()\n",
    "\n",
    "# Convert PeriodIndex to datetime for plotting\n",
    "avg_rating.index = avg_rating.index.to_timestamp()\n",
    "\n",
    "# Plot line chart\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(avg_rating.index, avg_rating.values, marker='o', color='blue')\n",
    "plt.title(\"Average Rating Over Time\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Average Rating\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NftDBhdt1qjg"
   },
   "outputs": [],
   "source": [
    "#Helpful votes\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Helpful flag: 1 = Helpful (votes > 10), 0 = Not Helpful\n",
    "df['helpful_flag'] = df['helpful_votes'].apply(lambda x: 1 if x > 10 else 0)\n",
    "\n",
    "# Count helpful vs not helpful\n",
    "helpful_counts = df['helpful_flag'].value_counts()\n",
    "\n",
    "# Map numeric flag to text labels\n",
    "label_map = {1: 'Helpful', 0: 'Not Helpful'}\n",
    "labels = [label_map[i] for i in helpful_counts.index]\n",
    "\n",
    "# Plot pie chart\n",
    "plt.figure(figsize=(6,6))\n",
    "plt.pie(helpful_counts,\n",
    "        labels=labels,\n",
    "        autopct='%1.1f%%',\n",
    "        startangle=90,\n",
    "        colors=['#4CAF50','#FF7043'])\n",
    "plt.title('Distribution of Helpful Reviews')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DQ400ex-1qgT"
   },
   "outputs": [],
   "source": [
    "#Average Rating by user location\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Group by location and calculate average rating\n",
    "avg_rating_by_location = df.groupby('location')['rating'].mean().sort_values(ascending=False)\n",
    "\n",
    "# Plot bar chart\n",
    "plt.figure(figsize=(12,6))\n",
    "avg_rating_by_location.plot(kind='bar', color='skyblue')\n",
    "plt.title(\"Average Rating by User Location\")\n",
    "plt.xlabel(\"Location\")\n",
    "plt.ylabel(\"Average Rating\")\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EjzZ8HzF1qdO"
   },
   "outputs": [],
   "source": [
    "# sentiment distribution trends\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Ensure date column is datetime\n",
    "df['date'] = pd.to_datetime(df['date'], errors='coerce')\n",
    "\n",
    "# Drop missing dates\n",
    "df = df.dropna(subset=['date'])\n",
    "\n",
    "# Extract only month names (Jan, Feb, Mar, ...)\n",
    "df['month'] = df['date'].dt.strftime('%b')\n",
    "\n",
    "# Order months correctly (not alphabetically)\n",
    "month_order = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']\n",
    "\n",
    "# Group by month and sentiment\n",
    "month_trend = df.groupby(['month', 'sentiment']).size().reset_index(name='count')\n",
    "\n",
    "# Ensure correct month order\n",
    "month_trend['month'] = pd.Categorical(month_trend['month'], categories=month_order, ordered=True)\n",
    "month_trend = month_trend.sort_values('month')\n",
    "\n",
    "# Plot month-wise sentiment trend\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.lineplot(\n",
    "    data=month_trend,\n",
    "    x='month',\n",
    "    y='count',\n",
    "    hue='sentiment',\n",
    "    marker='o',\n",
    "    palette={'Positive':'green', 'Negative':'red', 'Neutral':'blue'},\n",
    "    estimator='mean',\n",
    "    ci=None\n",
    ")\n",
    "\n",
    "plt.title('Month-wise Sentiment Trend (Aggregated)', fontsize=14, weight='bold')\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('Number of Reviews')\n",
    "plt.grid(True, linestyle='--', alpha=0.4)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "9ae8703e06ed4f67a6610a2391c2f33f",
      "09485c5783cd43cb90273de34b4c9a4d",
      "e285f19abbe0465bb96172af801057bd",
      "9297203a7b37407a9ba4b23e9e8519e1",
      "194e104d1fcf49f797bb9560e1de1b44",
      "b1e3bf0616794d7b8463b3dcaff5884f",
      "fee5f0684048413a8a943feb1a52fa3b",
      "202f633317764563a34938cae50d146a",
      "d98dcf5b0a6e4637af4c508cc9431dc6",
      "5bf17580a2f948ee8e410a93bab57e3c",
      "9969d3d5b6aa4ceda625fe2974e3eb40",
      "089aadc9730341e59d2f920b97d44e90",
      "4e33d9e8d412446e8b16b27794a55ad2",
      "a67e4e612fca497384e70297e7a4e71e",
      "bc750339a7054758ad3497c36c941b08",
      "fb53fc6de69e4dc484c3f33859d996d1",
      "856bdc3364914a7eb4d6cfdc0357ea73",
      "0e816fcbf55f4105baa02b7e1cc42bb8",
      "d2eba75873a04021b5db27e573437140",
      "e776822a3ef84e81978442824acf5db9",
      "c2c22ee0356d4f28a22178c9eeace4ca",
      "4c9f73d838544280a985f7d1c6e8cf80",
      "fd4553e982894777a3ab7579aaa6929c",
      "7fdc71beb75241b7829af826ea096b2b",
      "c8d02453514744a3b5fb27098c34245f",
      "e8dece1517a2401eb2d242756cf7f82b",
      "bcf7cca0113d469583cd9ad888266063",
      "0ed24e70030244baa5f6ec68a960f413",
      "106a0cca88df4be6ba48fbfd334b7c50",
      "bfbcc044bb864914bb44c93c2d210861",
      "3adca8b71d294ed5aac66ff4e74d9d8a",
      "04039f89fecd46a381bde1f91d14ef2d",
      "7fe74fb2f52347ab8fa831fcc4867302",
      "25b95ec2c84e43e8866f8fba17e81b34",
      "54bc0333709d4e93acb9360925b31735",
      "e3126953e4df41028518ceed3bc31726",
      "d8f92cdb4e7b441ea40e90768d9c18ed",
      "a4212dd1aeee425e903a7a51e321a7bc",
      "8f0531055afd419f91e63ddc08898a55",
      "8362782a375d4ad8a8b7868887b535fa",
      "38f61f4c805e423db11c2fc4f9cbf1be",
      "af71866cbcfe4180a74ab2c66177df99",
      "46481d3a04584ed4b43d938b1eeac2f2",
      "5e00f0a5c0a54bb09d76b15ef60cf6a6",
      "5b4038735e274a5cbd1ba52cc2843b70",
      "e171ad9e0c4149128c0f0c5877865fb7",
      "355360a4cfdb4284bcf5a84aa99866f1",
      "b9c77bbef0a747a3a2398720cb8b306e",
      "c0cda3dfd6ef4d8c80bb4b334b01b5a2",
      "28fda5adb8aa4f9691aa461d199faeec",
      "6e15335fbf9c4ed89cef4ababfc800e2",
      "d157c173b4064cfa9604702dc1b20274",
      "7537e9dd40c5463c9f902960a611b461",
      "c37c61f1fa864fc392aa3b7ea754b243",
      "b6934fe493964005b23f457a4ef7f30b",
      "af8a8dae27024b79bf5a0112b3e07375",
      "6d350f4872e54194a8065c0ec364e866",
      "66d7743a6a9c4c388cba7b2297e1c394",
      "0a6c3ef39286492d961277cba301cee5",
      "d446d5394fb14b998c090f7a10277203",
      "296b895eace84ff59909a0d5ed2075e7",
      "c65021db56d04c0fb9b4fc3fc20a58ea",
      "9f75a3d80beb415898637ef768e1334d",
      "261c7ab72eca44649aeba2a1e6a520da",
      "e217b71650aa43c6a0bde79649678830",
      "b4b459842c544c36bc9bac124964379d",
      "d40ee6087d7148e4a7c3633e95197a01",
      "a62f18e95c7b43f4b17ac2b16db10fa1",
      "31e02ab327ae498faa9b23888cac22a2",
      "1f7642f98e6d49f893917455320e8941",
      "ea3df7d0acf74aab9333eca9c6e051c5",
      "c0ade2ec4609418abfc8ca8e6c9d8b58",
      "02ce35be083b40db8ca84c0f77f6c906",
      "71d16f979b6b4e1f84414dc85eb99d33",
      "ba15befb85644587a663db4e703e8430",
      "044b886906ef4e35bd8ef437390ca67b",
      "4b7688565f4a4dff921b68772109de32",
      "36dfa1e491b0415e8e39646e7e51120d",
      "747c9560669b400cad33c72f43870610",
      "bf84cd221b5843bda992ac08a7fa9754",
      "3c095acc54e640b4a3192119c480ecd4",
      "6ba281c8a7dc4696b8eb30f01db49c90",
      "001d022c5c054ea8ad255eba5de57fd3",
      "0a334445b3c84d9dbcd30ef2915b5d49",
      "2fae6f126df44e9982405e4494f17061",
      "04a79a0d904d46f6a4258ec7cb2c3459",
      "bb2ece6a1889439ca8228537f8540c62",
      "45188bf658834d0393891ee2d6eb92f3",
      "8b0a96b434d24446995aa713ff264e37",
      "cdb5b51bf9ae47bcad11e5f5ae3ae46f",
      "71425191071648f4be5ba7ba8e193022",
      "9ad48b61c93049f99f25d3fb6c563c05",
      "70633ce5a4bd4962bee7b1d512c7ee53",
      "3f063bfddb4e4d6192f53489910693bf",
      "cd42ee217cb7416f833001d48c9522ce",
      "fc3d24b35fef4b55a9d4652b66b91b3e",
      "5a38a1ee12d647edb67d2dd397570941",
      "fca0e1826a3c4237b0fca41743d3ab12",
      "819617826377417ca70c313c0b9a1947",
      "b278218bb8f34152b9b6566c96e5103e",
      "8f52d5ca70524077a99da880a29efc6d",
      "1de6e04cabc344f3ad2f321128507bad",
      "378fc0b81b4443948c5f78e314a17587",
      "9494996a9ccd4acd94e068d18ee8101a",
      "9e944cf2023d41c3b3c0eb3b5b041fab",
      "7a10ffdc65b849309a2d557c29835f8c",
      "418ef0ca489c438b8d0136d907dfdba8",
      "1054f77cd5df408e8dec1700ede96115",
      "59e1c51b808e40e5850ff3d3f7ec6b54",
      "970b1c6a5b5940f3a0c25b4570be09aa",
      "2b95e83e4dbc41508d263445a46f32d1",
      "6e216587ea1e4fb492b59687fb1af1ec",
      "7640e823e54a4ec189eaf94edfcc174b",
      "cd4bd2c197c147ab88375c6784969d5d",
      "081aeb9db3f64b55a8cfa98d8b0774d9",
      "a974ed310bbb4348b5bc323e6e03739e",
      "f7870dfb3c7845c59e1aaed1a545fcd1",
      "7ec67770d507463f820e1f77b03482b8",
      "e443ccc4626944cc9536c38d120a74e5",
      "5549dbeb21394b1baad2fe0a115602ad",
      "0fb540bc9da3451bb07e8cba7b85280a",
      "eeaed47f652b4781aea2a699ab777ee8",
      "57f022c1025c416ca6c7165be79007d1",
      "231aa8f172d942dc9f782469a78bfb43",
      "3e40e406e2324ce8a79fbabfd2803bee",
      "d8ce02cb203b4b37a1ccd24d1666db90",
      "e9855ce0a63f446a9fb314d40e9a885d",
      "973ab77d4ca045a392aaa70b5ae08e76",
      "abe258c835284314be5671f2014f558e",
      "c64ab33071e34be9946b2f4314572742",
      "6fa4017cb18143e9b014d9a99248bb05",
      "f2176f1e11bf435f96c8038439bf0175",
      "5187b3a3bc324b4fb743b78481b31d5f",
      "c10fe2fdb412407da5a287bf5d893e30",
      "0ad29aa4417a4c4eb6899f28ba978c4a",
      "f1676393b7614245a7b59e5dcdd3279a",
      "e40022719aa54cc59a04dd9211164e66",
      "35867935bd3f460d85fd7e427a18fc6f",
      "ce4d5b526d8841a4aa879dcb842839c4",
      "83a781013b364bcbb8161c802d1bca2e",
      "4daf43423ff042ba978d896db8224b25",
      "d2001eeb0f3e449abf6e9d2b5c24c70e",
      "e3c2abfe70b442d7af69a4ad45cf2bb9",
      "beb6030cc42e456ebc6b678cf1a35324",
      "f3e655774b8c4557a325c42a1b71ab1d",
      "53deae9fb9714764b1c68aa2c12aabde",
      "01ffb2272df641be86eadb9cf9a2b118",
      "01f94d7f9f2b4f63bb212838441aead6",
      "5c463f6aa08e4404a1fbf8a7d6287e23",
      "d4e66432029449cf8462918d2168a9e1",
      "9705d5756dde447590a90138b6764073",
      "32d6cbd589c1489bbaa8223d268b0d41",
      "13c0c3169e444b93a4f5b11ee2a9d2f9",
      "67cc42843b4d41cf91c60d8ceed6e317",
      "4705cf551a114c69aea7c19d1ca83139",
      "01b38d95800548aea1001e68c4009aa1",
      "75ff5e9da41246938ebab529b820844f",
      "133edfdc769d4d47bff1a2a1c7ef101e",
      "119326418dfb46ea9a19dcb68d54fb4b",
      "3dc48b27ef444c32bded98bc1d13c9a3",
      "6d438ca68e3544eea8e610132c28eed9",
      "55af773050b7458d9bc7bc9602920550",
      "5840d43d4d19498781b258964d031028",
      "aaa39b3c9cd84255b9223fdb94e9aff8",
      "63ff1916c3f84d5cb14fab8b46376931",
      "8bd3b60b572d45cab6edde4a5408fdf9",
      "0f66bfa192db416087e1de1ab883b8f1",
      "d4e813145d1b483581e38fbdff4741ef",
      "562ff8c568304b70a1b9cd5aea803f52",
      "d55c4e7ab37e4b36938d7cde062dcf24",
      "3ff323093b684446a4442239b8c9e1a3",
      "55d0e9e486df4a5faec3f381dc9f6444",
      "07a324096e734d538e5fd467d4ec04f7",
      "18ec472b50924093920703a0fbb7aba9",
      "f94ce61f0fc74963b1cbbc4394f42fe9",
      "b7f71ccee30c4a729bffa5bb76b6ab85",
      "bbfba092fcf5463db78dffaa4d1503f3",
      "44fc14a8d78744dd822548597b8946f2",
      "08dde2acc8734fef87ef0c1bf9c9a0bf",
      "30c698ae2c854e45b5fa45551dba3a5b",
      "886c5e611c904bbe855641e444014e36",
      "0ec9ebbd109e4669b8ded90d9c7a8130",
      "195895bd6d3340e4baab6f7beb057d01",
      "b666a1be4d1647db83c101ce4ae131d5",
      "6079ccdcd9ff4d46ac383cc4c45d946a",
      "4f2f4d0ee12848fe9a9058c030a3c3e0",
      "cc7806d9e5354e9897cbcf12ea65b775",
      "36fe5a76bda04968b9c7332d0bf65ca8",
      "c3e40305185a45c88c49975a64883842",
      "3752f1c90ed74b3a8b40a86dcd5a58ed",
      "053ed15c51284ffdb3d78b0f77d845e1",
      "22c0d43a7cee42bd9bb62256e5799e2c",
      "43cfc14ef5c34ad1b2b1fa9de1fbad6b",
      "03598f1584644c369fff00a5ff64085d",
      "c12863fc4778447d9e3d1b8e5bee1811",
      "fc26fefd331b44529e009e9349640334",
      "e711245260ac49cfb05bbba8f9bc1b1e",
      "c6311833aa84493ea06447e9cdd29c65"
     ]
    },
    "id": "Wg4jNBNoqSS0",
    "outputId": "aaad191c-eaa3-47eb-cf75-40193c7e3235"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
      "Requirement already satisfied: seaborn in /usr/local/lib/python3.12/dist-packages (0.13.2)\n",
      "Collecting langdetect\n",
      "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
      "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/981.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.2/981.5 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: imbalanced-learn in /usr/local/lib/python3.12/dist-packages (0.14.0)\n",
      "Collecting streamlit\n",
      "  Downloading streamlit-1.51.0-py3-none-any.whl.metadata (9.5 kB)\n",
      "Collecting nlpaug\n",
      "  Downloading nlpaug-1.1.11-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting pyngrok\n",
      "  Downloading pyngrok-7.5.0-py3-none-any.whl.metadata (8.1 kB)\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.1)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.3.1)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.60.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
      "Requirement already satisfied: numpy>=1.23 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (11.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.2.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: pandas>=1.2 in /usr/local/lib/python3.12/dist-packages (from seaborn) (2.2.2)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.12/dist-packages (from langdetect) (1.17.0)\n",
      "Requirement already satisfied: scipy<2,>=1.11.4 in /usr/local/lib/python3.12/dist-packages (from imbalanced-learn) (1.16.3)\n",
      "Requirement already satisfied: scikit-learn<2,>=1.4.2 in /usr/local/lib/python3.12/dist-packages (from imbalanced-learn) (1.6.1)\n",
      "Requirement already satisfied: threadpoolctl<4,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from imbalanced-learn) (3.6.0)\n",
      "Requirement already satisfied: altair!=5.4.0,!=5.4.1,<6,>=4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (5.5.0)\n",
      "Requirement already satisfied: blinker<2,>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (1.9.0)\n",
      "Requirement already satisfied: cachetools<7,>=4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (5.5.2)\n",
      "Requirement already satisfied: protobuf<7,>=3.20 in /usr/local/lib/python3.12/dist-packages (from streamlit) (5.29.5)\n",
      "Requirement already satisfied: pyarrow<22,>=7.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (18.1.0)\n",
      "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.12/dist-packages (from streamlit) (2.32.4)\n",
      "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (8.5.0)\n",
      "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.12/dist-packages (from streamlit) (0.10.2)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (4.15.0)\n",
      "Requirement already satisfied: watchdog<7,>=2.1.5 in /usr/local/lib/python3.12/dist-packages (from streamlit) (6.0.0)\n",
      "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.12/dist-packages (from streamlit) (3.1.45)\n",
      "Collecting pydeck<1,>=0.8.0b4 (from streamlit)\n",
      "  Downloading pydeck-0.9.1-py2.py3-none-any.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: tornado!=6.5.0,<7,>=6.0.3 in /usr/local/lib/python3.12/dist-packages (from streamlit) (6.5.1)\n",
      "Requirement already satisfied: gdown>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from nlpaug) (5.2.0)\n",
      "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.12/dist-packages (from pyngrok) (6.0.3)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.36.0)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.7.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (3.1.6)\n",
      "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.12/dist-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (4.25.1)\n",
      "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.12/dist-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (2.12.0)\n",
      "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.12/dist-packages (from gdown>=4.0.0->nlpaug) (4.13.5)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.3.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.2->seaborn) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.2->seaborn) (2025.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (2025.11.12)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (3.0.3)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (25.4.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (2025.9.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (0.37.0)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (0.29.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4->gdown>=4.0.0->nlpaug) (2.8)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.12/dist-packages (from requests[socks]->gdown>=4.0.0->nlpaug) (1.7.1)\n",
      "Downloading streamlit-1.51.0-py3-none-any.whl (10.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m64.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nlpaug-1.1.11-py3-none-any.whl (410 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.5/410.5 kB\u001b[0m \u001b[31m26.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyngrok-7.5.0-py3-none-any.whl (24 kB)\n",
      "Downloading pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m58.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: langdetect\n",
      "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993223 sha256=0fdb6c6f6ae26e4126b4871a58b03039fdeecf65fd6af20d397840f8e34fae1d\n",
      "  Stored in directory: /root/.cache/pip/wheels/c1/67/88/e844b5b022812e15a52e4eaa38a1e709e99f06f6639d7e3ba7\n",
      "Successfully built langdetect\n",
      "Installing collected packages: pyngrok, langdetect, pydeck, nlpaug, streamlit\n",
      "Successfully installed langdetect-1.0.9 nlpaug-1.1.11 pydeck-0.9.1 pyngrok-7.5.0 streamlit-1.51.0\n",
      "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.12/dist-packages (5.1.2)\n",
      "Requirement already satisfied: wordcloud in /usr/local/lib/python3.12/dist-packages (1.9.4)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.57.1)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.67.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (2.9.0+cu126)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.6.1)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.16.3)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (0.36.0)\n",
      "Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (11.3.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.15.0)\n",
      "Requirement already satisfied: numpy>=1.6.1 in /usr/local/lib/python3.12/dist-packages (from wordcloud) (2.0.2)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from wordcloud) (3.10.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.20.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.3)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.4)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.2.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (75.2.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.3)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.3.20)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.5.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.7.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->wordcloud) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->wordcloud) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->wordcloud) (4.60.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->wordcloud) (1.4.9)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->wordcloud) (3.2.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib->wordcloud) (2.9.0.post0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib->wordcloud) (1.17.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2025.11.12)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original distribution:\n",
      " sentiment\n",
      "Negative    20\n",
      "Positive    17\n",
      "Neutral     13\n",
      "Name: count, dtype: int64 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ae8703e06ed4f67a6610a2391c2f33f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "089aadc9730341e59d2f920b97d44e90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd4553e982894777a3ab7579aaa6929c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25b95ec2c84e43e8866f8fba17e81b34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b4038735e274a5cbd1ba52cc2843b70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following layers were not sharded: bert.embeddings.LayerNorm.bias, cls.predictions.decoder.weight, bert.encoder.layer.*.output.LayerNorm.weight, cls.predictions.decoder.bias, bert.encoder.layer.*.attention.output.LayerNorm.bias, cls.predictions.transform.dense.weight, bert.encoder.layer.*.output.dense.weight, bert.encoder.layer.*.attention.self.key.weight, bert.encoder.layer.*.intermediate.dense.weight, bert.encoder.layer.*.intermediate.dense.bias, bert.encoder.layer.*.output.dense.bias, cls.predictions.transform.LayerNorm.bias, cls.predictions.transform.LayerNorm.weight, bert.encoder.layer.*.attention.self.query.weight, bert.encoder.layer.*.attention.self.value.bias, cls.predictions.transform.dense.bias, bert.embeddings.LayerNorm.weight, bert.encoder.layer.*.attention.self.value.weight, bert.encoder.layer.*.attention.self.query.bias, bert.encoder.layer.*.attention.output.LayerNorm.weight, cls.predictions.bias, bert.embeddings.word_embeddings.weight, bert.encoder.layer.*.output.LayerNorm.bias, bert.encoder.layer.*.attention.self.key.bias, bert.embeddings.token_type_embeddings.weight, bert.encoder.layer.*.attention.output.dense.weight, bert.encoder.layer.*.attention.output.dense.bias, bert.embeddings.position_embeddings.weight\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final counts after augmentation:\n",
      "\n",
      "sentiment\n",
      "Negative    60\n",
      "Positive    51\n",
      "Neutral     39\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Label mapping: {np.int64(0): 'Negative', np.int64(1): 'Neutral', np.int64(2): 'Positive'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af8a8dae27024b79bf5a0112b3e07375",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d40ee6087d7148e4a7c3633e95197a01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36dfa1e491b0415e8e39646e7e51120d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b0a96b434d24446995aa713ff264e37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b278218bb8f34152b9b6566c96e5103e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b95e83e4dbc41508d263445a46f32d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following layers were not sharded: encoder.layer.*.intermediate.dense.bias, encoder.layer.*.output.dense.weight, embeddings.position_embeddings.weight, encoder.layer.*.attention.output.LayerNorm.bias, embeddings.LayerNorm.weight, pooler.dense.bias, embeddings.token_type_embeddings.weight, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.bias, encoder.layer.*.attention.self.value.bias, encoder.layer.*.attention.self.key.weight, pooler.dense.weight, encoder.layer.*.attention.self.value.weight, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.output.dense.bias, embeddings.word_embeddings.weight, encoder.layer.*.attention.self.query.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.self.key.bias, encoder.layer.*.output.LayerNorm.weight\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eeaed47f652b4781aea2a699ab777ee8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5187b3a3bc324b4fb743b78481b31d5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "beb6030cc42e456ebc6b678cf1a35324",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4705cf551a114c69aea7c19d1ca83139",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8bd3b60b572d45cab6edde4a5408fdf9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbfba092fcf5463db78dffaa4d1503f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36fe5a76bda04968b9c7332d0bf65ca8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.70      0.58      0.64        12\n",
      "     Neutral       0.60      0.38      0.46         8\n",
      "    Positive       0.53      0.80      0.64        10\n",
      "\n",
      "    accuracy                           0.60        30\n",
      "   macro avg       0.61      0.59      0.58        30\n",
      "weighted avg       0.62      0.60      0.59        30\n",
      "\n",
      "\n",
      "🎯 Accuracy: 60.00%\n",
      "Macro F1: 0.5793006993006994\n",
      "ROC-AUC: 0.7616442199775534\n",
      "I am not sure what to think about this product. → Neutral\n",
      "Wow, what a fantastic product! → Positive\n",
      "I am very disappointed with the product. → Negative\n",
      "I don't love it, I don't hate it. It is just okay. → Neutral\n"
     ]
    }
   ],
   "source": [
    "###fully corrected - MiniLM + SBERT + Logistic Regression\n",
    "!pip install nltk matplotlib seaborn langdetect imbalanced-learn streamlit nlpaug pyngrok transformers\n",
    "!pip install sentence-transformers wordcloud\n",
    "\n",
    "\n",
    "# ===============================================================\n",
    "# 2. IMPORTS\n",
    "# ===============================================================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from langdetect import detect\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nlpaug.augmenter.sentence as nas\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.metrics import classification_report, f1_score, roc_auc_score, accuracy_score\n",
    "\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import nlpaug.augmenter.word as naw\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"wordnet\")\n",
    "\n",
    "# ===============================================================\n",
    "# 3. REQUIRED FUNCTIONS\n",
    "# ===============================================================\n",
    "def is_english(text):\n",
    "    try:\n",
    "        return detect(text) == \"en\"\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "def clean_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"http\\S+|www\\S+\", \"\", text)\n",
    "    text = re.sub(r\"[^a-zA-Z\\s]\", \" \", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    words = [lemmatizer.lemmatize(w) for w in text.split() if w not in stop_words]\n",
    "    return \" \".join(words)\n",
    "\n",
    "# ===============================================================\n",
    "# 4. LOAD YOUR DATA\n",
    "# ===============================================================\n",
    "DATA_PATH = \"/content/chatgpt_review_dataset.csv\"\n",
    "\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "df = df.dropna(subset=[\"review\", \"rating\"])\n",
    "df = df[df[\"review\"].apply(is_english)].reset_index(drop=True)\n",
    "\n",
    "df[\"cleaned_text\"] = df[\"review\"].apply(clean_text)\n",
    "df[\"cleaned_text\"] = df[\"cleaned_text\"].astype(str).str.strip()\n",
    "df = df[df[\"cleaned_text\"].str.len() > 0].reset_index(drop=True)\n",
    "\n",
    "# Create sentiment from rating if missing\n",
    "if \"sentiment\" not in df.columns:\n",
    "    def rating_to_sentiment(r):\n",
    "        try:\n",
    "            r = float(r)\n",
    "        except:\n",
    "            return None\n",
    "        if r >= 4:\n",
    "            return \"Positive\"\n",
    "        elif r == 3:\n",
    "            return \"Neutral\"\n",
    "        else:\n",
    "            return \"Negative\"\n",
    "    df[\"sentiment\"] = df[\"rating\"].apply(rating_to_sentiment)\n",
    "\n",
    "df = df.dropna(subset=[\"sentiment\"]).reset_index(drop=True)\n",
    "print(\"Original distribution:\\n\", df[\"sentiment\"].value_counts(), \"\\n\")\n",
    "\n",
    "# ------------------------------------------\n",
    "# 5. AUGMENTATION (Safe Contextual + SBERT)\n",
    "# ------------------------------------------\n",
    "aug1 = naw.ContextualWordEmbsAug(\n",
    "    model_path=\"bert-base-uncased\",\n",
    "    action=\"substitute\",\n",
    "    aug_p=0.25\n",
    ")\n",
    "\n",
    "aug2 = naw.ContextualWordEmbsAug(\n",
    "    model_path=\"bert-base-uncased\",\n",
    "    action=\"insert\",\n",
    "    aug_p=0.15\n",
    ")\n",
    "\n",
    "para_aug = naw.SynonymAug(\n",
    "    model_path=\"sentence-transformers/paraphrase-MiniLM-L6-v2\"\n",
    ")\n",
    "\n",
    "aug_texts, aug_labels = [], []\n",
    "\n",
    "def to_clean_string(x):\n",
    "    \"\"\"Convert any augmentation output into a valid cleaned string\"\"\"\n",
    "    if isinstance(x, list):\n",
    "        x = \" \".join(map(str, x))\n",
    "    if not isinstance(x, str):\n",
    "        return \"\"\n",
    "    return x.strip()\n",
    "\n",
    "for i, row in df.iterrows():\n",
    "    text = row[\"cleaned_text\"]\n",
    "    label = row[\"sentiment\"]\n",
    "    for augmenter in [aug1, aug2, para_aug]:\n",
    "        try:\n",
    "            out = augmenter.augment(text)\n",
    "            clean_out = to_clean_string(out)\n",
    "            if clean_out and len(clean_out) > 3:\n",
    "                aug_texts.append(clean_out)\n",
    "                aug_labels.append(label)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "aug_df = pd.DataFrame({\"cleaned_text\": aug_texts, \"sentiment\": aug_labels})\n",
    "df_final = pd.concat([df, aug_df], ignore_index=True)\n",
    "\n",
    "df_final[\"cleaned_text\"] = df_final[\"cleaned_text\"].astype(str).str.strip()\n",
    "df_final = df_final[df_final[\"cleaned_text\"].str.len() > 0].reset_index(drop=True)\n",
    "\n",
    "print(\"\\nFinal counts after augmentation:\\n\")\n",
    "print(df_final[\"sentiment\"].value_counts())\n",
    "\n",
    "# ===============================================================\n",
    "# 6. LABEL ENCODING\n",
    "# ===============================================================\n",
    "le = LabelEncoder()\n",
    "df_final[\"label\"] = le.fit_transform(df_final[\"sentiment\"])\n",
    "print(\"\\nLabel mapping:\", dict(zip(le.transform(le.classes_), le.classes_)))\n",
    "\n",
    "# ===============================================================\n",
    "# 7. TRAIN/TEST SPLIT\n",
    "# ===============================================================\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df_final[\"cleaned_text\"], df_final[\"label\"],\n",
    "    test_size=0.2, stratify=df_final[\"label\"], random_state=42\n",
    ")\n",
    "\n",
    "# ===============================================================\n",
    "# 8. EMBEDDINGS (MiniLM)\n",
    "# ===============================================================\n",
    "embedder = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "X_train_vec = embedder.encode(X_train.tolist(), convert_to_numpy=True, show_progress_bar=True)\n",
    "X_test_vec = embedder.encode(X_test.tolist(), convert_to_numpy=True, show_progress_bar=True)\n",
    "\n",
    "# ===============================================================\n",
    "# 9. CLASSIFIER + CALIBRATION\n",
    "# ===============================================================\n",
    "clf = LogisticRegression(\n",
    "    max_iter=2000,\n",
    "    class_weight=\"balanced\",\n",
    "    multi_class=\"ovr\"\n",
    ")\n",
    "calibrated_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n",
    "calibrated_clf.fit(X_train_vec, y_train)\n",
    "\n",
    "# ===============================================================\n",
    "# 10. EVALUATION\n",
    "# ===============================================================\n",
    "preds = calibrated_clf.predict(X_test_vec)\n",
    "probs = calibrated_clf.predict_proba(X_test_vec)\n",
    "\n",
    "print(\"\\nClassification Report:\\n\",\n",
    "      classification_report(y_test, preds, target_names=le.classes_))\n",
    "\n",
    "print(f\"\\n🎯 Accuracy: {accuracy_score(y_test, preds)*100:.2f}%\")\n",
    "print(\"Macro F1:\", f1_score(y_test, preds, average=\"macro\"))\n",
    "print(\"ROC-AUC:\", roc_auc_score(y_test, probs, multi_class=\"ovr\"))\n",
    "\n",
    "# ===============================================================\n",
    "# 11. HYBRID NEUTRAL-SAFE DECISION ENGINE (REVISED)\n",
    "# ===============================================================\n",
    "PROBA_UNCERTAIN = 0.50          # below this, call Neutral\n",
    "NEU_MARGIN = 0.20               # |neg - pos| closeness for Neutral\n",
    "NEU_MIN = 0.15                  # minimal neutral proba to allow Neutral\n",
    "NEU_TOP_GAP = 0.10              # if Neutral is close to max, call Neutral\n",
    "\n",
    "NEUTRAL_CUES = {\n",
    "    \"not sure\", \"unsure\", \"mixed\", \"ambivalent\", \"meh\", \"okay\", \"ok\",\n",
    "    \"just okay\", \"fine\", \"average\", \"so so\", \"so-so\", \"decent\",\n",
    "    \"i don't love it\", \"i dont love it\", \"i don't hate it\", \"i dont hate it\",\n",
    "    \"it is just okay\", \"it’s just okay\", \"it is okay\", \"it’s okay\"\n",
    "}\n",
    "POSITIVE_CUES = {\n",
    "    \"fantastic\", \"great\", \"excellent\", \"amazing\", \"love\", \"wonderful\",\n",
    "    \"awesome\", \"superb\", \"terrific\"\n",
    "}\n",
    "NEGATIVE_CUES = {\n",
    "    \"disappointed\", \"bad\", \"terrible\", \"awful\", \"hate\", \"poor\", \"worse\",\n",
    "    \"not good\", \"mediocre\", \"buggy\"\n",
    "}\n",
    "\n",
    "def _has_cue(text, cues):\n",
    "    t = text.lower()\n",
    "    return any(c in t for c in cues)\n",
    "\n",
    "def hybrid_predict(text, debug=False):\n",
    "    cleaned = clean_text(text)\n",
    "    vec = embedder.encode([cleaned], convert_to_numpy=True)\n",
    "    proba = calibrated_clf.predict_proba(vec)[0]\n",
    "\n",
    "    proba_dict = dict(zip(le.classes_, proba))\n",
    "    neg_p = proba_dict.get(\"Negative\", 0.0)\n",
    "    neu_p = proba_dict.get(\"Neutral\", 0.0)\n",
    "    pos_p = proba_dict.get(\"Positive\", 0.0)\n",
    "\n",
    "    max_p = max(proba)\n",
    "    pred_idx = np.argmax(proba)\n",
    "    pred_label = le.inverse_transform([pred_idx])[0]\n",
    "\n",
    "    # 1) Strong lexical cues override probabilities\n",
    "    if _has_cue(text, POSITIVE_CUES) and not _has_cue(text, NEGATIVE_CUES):\n",
    "        return \"Positive\"\n",
    "    if _has_cue(text, NEGATIVE_CUES) and not _has_cue(text, POSITIVE_CUES):\n",
    "        return \"Negative\"\n",
    "    # Neutral cue wins unless there is a very strong class (>0.65)\n",
    "    if _has_cue(text, NEUTRAL_CUES) and max_p < 0.65:\n",
    "        return \"Neutral\"\n",
    "\n",
    "    # 2) Uncertainty fallback\n",
    "    if max_p < PROBA_UNCERTAIN:\n",
    "        return \"Neutral\"\n",
    "\n",
    "    # 3) Pos-Neg tie → Neutral if Neutral is non-trivial\n",
    "    if abs(neg_p - pos_p) < NEU_MARGIN and neu_p > NEU_MIN:\n",
    "        return \"Neutral\"\n",
    "\n",
    "    # 4) Neutral near top → Neutral\n",
    "    if (max_p - neu_p) < NEU_TOP_GAP and neu_p > NEU_MIN:\n",
    "        return \"Neutral\"\n",
    "\n",
    "    if debug:\n",
    "        print(f\"Probabilities: {proba_dict}\")\n",
    "    return pred_label\n",
    "\n",
    "# ===============================================================\n",
    "# 12. BATCH PREDICT\n",
    "# ===============================================================\n",
    "def predict_list(samples):\n",
    "    for s in samples:\n",
    "        print(f\"{s} → {hybrid_predict(s, debug=True)}\")\n",
    "\n",
    "# ===============================================================\n",
    "# 13. TEST\n",
    "# ===============================================================\n",
    "samples = [\n",
    "    \"I am not sure what to think about this product.\",\n",
    "    \"Wow, what a fantastic product!\",\n",
    "    \"I am very disappointed with the product.\",\n",
    "    \"I don't love it, I don't hate it. It is just okay.\"\n",
    "]\n",
    "predict_list(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MtURlRHewCzb"
   },
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# Save artifacts\n",
    "joblib.dump(le, \"label_encoder.pkl\")\n",
    "joblib.dump(calibrated_clf, \"calibrated_clf.pkl\")\n",
    "\n",
    "# Save embedder name\n",
    "with open(\"embedder_name.txt\", \"w\") as f:\n",
    "    f.write(\"sentence-transformers/all-MiniLM-L6-v2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4ms8tt82W6YF",
    "outputId": "7761ecf4-3c84-4c49-decc-6c28414d8993"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /content/trans_streamlitapp1.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile /content/trans_streamlitapp1.py\n",
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "import re\n",
    "import nltk\n",
    "from wordcloud import WordCloud\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"wordnet\")\n",
    "\n",
    "# ===============================================================\n",
    "# 1. PAGE CONFIG + DESIGN\n",
    "# ===============================================================\n",
    "st.set_page_config(page_title=\"Sentiment Analysis & EDA\", layout=\"wide\")\n",
    "\n",
    "st.markdown(\"\"\"\n",
    "<style>\n",
    "[data-testid=\"stAppViewContainer\"] {\n",
    "    background: linear-gradient(135deg, #f0f4f8, #d9e4f5);\n",
    "}\n",
    "[data-testid=\"stSidebar\"] {\n",
    "    background-color: #f8f9fa !important;\n",
    "    color: #2c3e50;\n",
    "}\n",
    "[data-testid=\"stSidebar\"] .css-1v3fvcr {\n",
    "    color: #2c3e50 !important;\n",
    "}\n",
    "h1, h2, h3 {\n",
    "    color: #2c3e50;\n",
    "}\n",
    ".stButton>button {\n",
    "    background-color: #3498db;\n",
    "    color: white;\n",
    "    border-radius: 8px;\n",
    "}\n",
    ".stButton>button:hover {\n",
    "    background-color: #2980b9;\n",
    "}\n",
    "</style>\n",
    "\"\"\", unsafe_allow_html=True)\n",
    "\n",
    "\n",
    "# ===============================================================\n",
    "# 2. LOAD ARTIFACTS\n",
    "# ===============================================================\n",
    "le = joblib.load(\"label_encoder.pkl\")\n",
    "calibrated_clf = joblib.load(\"calibrated_clf.pkl\")\n",
    "with open(\"embedder_name.txt\") as f:\n",
    "    embedder_name = f.read().strip()\n",
    "embedder = SentenceTransformer(embedder_name)\n",
    "\n",
    "# ===============================================================\n",
    "# 3. CLEANING + HYBRID PREDICT\n",
    "# ===============================================================\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "def clean_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"http\\S+|www\\S+\", \"\", text)\n",
    "    text = re.sub(r\"[^a-zA-Z\\s]\", \" \", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    words = [lemmatizer.lemmatize(w) for w in text.split() if w not in stop_words]\n",
    "    return \" \".join(words)\n",
    "\n",
    "PROBA_UNCERTAIN = 0.50\n",
    "NEU_MARGIN = 0.20\n",
    "NEU_MIN = 0.15\n",
    "NEU_TOP_GAP = 0.10\n",
    "\n",
    "NEUTRAL_CUES = {\n",
    "    \"not sure\", \"unsure\", \"mixed\", \"ambivalent\", \"meh\", \"okay\", \"ok\",\n",
    "    \"just okay\", \"fine\", \"average\", \"so so\", \"so-so\", \"decent\",\n",
    "    \"i don't love it\", \"i dont love it\", \"i don't hate it\", \"i dont hate it\",\n",
    "    \"it is just okay\", \"it’s just okay\", \"it is okay\", \"it’s okay\"\n",
    "}\n",
    "POSITIVE_CUES = {\n",
    "    \"fantastic\", \"great\", \"excellent\", \"amazing\", \"love\", \"wonderful\",\n",
    "    \"awesome\", \"superb\", \"terrific\"\n",
    "}\n",
    "NEGATIVE_CUES = {\n",
    "    \"disappointed\", \"bad\", \"terrible\", \"awful\", \"hate\", \"poor\", \"worse\",\n",
    "    \"not good\", \"mediocre\", \"buggy\"\n",
    "}\n",
    "\n",
    "def _has_cue(text, cues):\n",
    "    t = text.lower()\n",
    "    return any(c in t for c in cues)\n",
    "\n",
    "def hybrid_predict(text):\n",
    "    cleaned = clean_text(text)\n",
    "    vec = embedder.encode([cleaned], convert_to_numpy=True)\n",
    "    proba = calibrated_clf.predict_proba(vec)[0]\n",
    "    proba_dict = dict(zip(le.classes_, proba))\n",
    "\n",
    "    neg_p = proba_dict.get(\"Negative\", 0.0)\n",
    "    neu_p = proba_dict.get(\"Neutral\", 0.0)\n",
    "    pos_p = proba_dict.get(\"Positive\", 0.0)\n",
    "\n",
    "    max_p = max(proba)\n",
    "    pred_idx = np.argmax(proba)\n",
    "    pred_label = le.inverse_transform([pred_idx])[0]\n",
    "\n",
    "    if _has_cue(text, POSITIVE_CUES) and not _has_cue(text, NEGATIVE_CUES):\n",
    "        return \"Positive\"\n",
    "    if _has_cue(text, NEGATIVE_CUES) and not _has_cue(text, POSITIVE_CUES):\n",
    "        return \"Negative\"\n",
    "    if _has_cue(text, NEUTRAL_CUES) and max_p < 0.65:\n",
    "        return \"Neutral\"\n",
    "    if max_p < PROBA_UNCERTAIN:\n",
    "        return \"Neutral\"\n",
    "    if abs(neg_p - pos_p) < NEU_MARGIN and neu_p > NEU_MIN:\n",
    "        return \"Neutral\"\n",
    "    if (max_p - neu_p) < NEU_TOP_GAP and neu_p > NEU_MIN:\n",
    "        return \"Neutral\"\n",
    "\n",
    "    return pred_label\n",
    "\n",
    "\n",
    "# ===============================================================\n",
    "# 4. NAVIGATION\n",
    "# ===============================================================\n",
    "page = st.sidebar.radio(\"📌 Navigation\", [\"Sentiment Analysis\", \"Charts\"])\n",
    "\n",
    "\n",
    "# ===============================================================\n",
    "# 5. SENTIMENT ANALYSIS PAGE\n",
    "# ===============================================================\n",
    "if page == \"Sentiment Analysis\":\n",
    "\n",
    "    st.title(\"🔮Sentiment Analysis\")\n",
    "    st.write(\"Enter a product review and get the predicted sentiment.\")\n",
    "\n",
    "    user_input = st.text_area(\"✍️ Enter your review:\", \"\")\n",
    "\n",
    "    if st.button(\"Analyze\"):\n",
    "        if user_input.strip():\n",
    "            label = hybrid_predict(user_input)\n",
    "            st.success(f\"Predicted Sentiment: **{label}**\")\n",
    "        else:\n",
    "            st.warning(\"Please enter some text.\")\n",
    "\n",
    "\n",
    "# ===============================================================\n",
    "# 6. CHARTS PAGE\n",
    "# ===============================================================\n",
    "elif page == \"Charts\":\n",
    "\n",
    "    st.title(\"Exploratory Data Analysis Charts\")\n",
    "\n",
    "    DATA_PATH = \"/content/chatgpt_review_dataset.csv\"\n",
    "    df = pd.read_csv(DATA_PATH)\n",
    "    df = df.dropna(subset=[\"review\", \"rating\"])\n",
    "    df[\"sentiment\"] = df[\"rating\"].apply(lambda r: \"Positive\" if r >= 4 else (\"Neutral\" if r == 3 else \"Negative\"))\n",
    "    df[\"cleaned_text\"] = df[\"review\"].astype(str).apply(clean_text)\n",
    "\n",
    "    chart_option = st.sidebar.radio(\n",
    "        \"Select Chart\",\n",
    "        [\n",
    "            \"Overall Distribution\",\n",
    "            \"Sentiment vs Rating\",\n",
    "            \"Word Clouds per Sentiment\",\n",
    "            \"Sentiment Over Time\",\n",
    "            \"Verified vs Non-Verified\",\n",
    "            \"Review length vs Sentiment\",\n",
    "            \"Sentiment by Location\",\n",
    "            \"Platform Sentiment\",\n",
    "            \"Sentiment by ChatGptVersion\",\n",
    "            \"Topic Modeling on Negative Reviews\"\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # =======================\n",
    "    # 1. Overall Distribution\n",
    "    # =======================\n",
    "    if chart_option == \"Overall Distribution\":\n",
    "        fig, ax = plt.subplots(figsize=(4, 3))\n",
    "        sns.countplot(x=\"sentiment\", data=df, palette=\"magma\", ax=ax)\n",
    "        ax.set_title(\"Overall Sentiment Distribution\")\n",
    "        st.pyplot(fig)\n",
    "\n",
    "    # =======================\n",
    "    # 2. Sentiment vs Rating\n",
    "    # =======================\n",
    "    elif chart_option == \"Sentiment vs Rating\":\n",
    "        sentiment_by_rating = df.groupby(\"rating\")[\"sentiment\"].value_counts(normalize=True).unstack()\n",
    "        fig, ax = plt.subplots(figsize=(4, 3))\n",
    "        sentiment_by_rating.plot(kind=\"bar\", ax=ax)\n",
    "        ax.set_title(\"Sentiment Distribution Across Ratings\", fontsize=8)\n",
    "        ax.set_xlabel(\"Rating\", fontsize=6)\n",
    "        ax.set_ylabel(\"Proportion\", fontsize=6)\n",
    "        ax.tick_params(axis=\"x\", labelsize=5, rotation=0)\n",
    "        ax.tick_params(axis=\"y\", labelsize=5)\n",
    "        ax.legend(title=\"Sentiment\", fontsize=6, title_fontsize=6)\n",
    "        plt.tight_layout()\n",
    "        st.pyplot(fig)\n",
    "\n",
    "    # =======================\n",
    "    # 3. Word Clouds\n",
    "    # =======================\n",
    "    elif chart_option == \"Word Clouds per Sentiment\":\n",
    "        tabs = st.tabs([\"Positive\", \"Neutral\", \"Negative\"])\n",
    "        for sentiment_name, tab in zip([\"Positive\", \"Neutral\", \"Negative\"], tabs):\n",
    "            with tab:\n",
    "                text = \" \".join(df[df[\"sentiment\"] == sentiment_name][\"cleaned_text\"])\n",
    "                wc = WordCloud(width=400, height=200, background_color=\"white\").generate(text)\n",
    "                fig, ax = plt.subplots(figsize=(3, 2))\n",
    "                ax.imshow(wc, interpolation=\"bilinear\")\n",
    "                ax.axis(\"off\")\n",
    "                st.pyplot(fig)\n",
    "\n",
    "    # =======================\n",
    "    # 4. Sentiment Over Time\n",
    "    # =======================\n",
    "    elif chart_option == \"Sentiment Over Time\":\n",
    "        df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
    "        df[\"month\"] = df[\"date\"].dt.to_period(\"M\").astype(str)\n",
    "        monthly_sentiment = df.groupby([\"month\", \"sentiment\"]).size().reset_index(name=\"count\")\n",
    "        pivot = monthly_sentiment.pivot(index=\"month\", columns=\"sentiment\", values=\"count\").fillna(0)\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(5, 2))\n",
    "        ax.plot(pivot.index, pivot[\"Positive\"], marker=\"o\")\n",
    "        ax.plot(pivot.index, pivot[\"Negative\"], marker=\"o\")\n",
    "        ax.plot(pivot.index, pivot[\"Neutral\"], marker=\"o\")\n",
    "\n",
    "        ax.set_title(\"Sentiment Trend Over Time (Monthly)\")\n",
    "        ax.set_xlabel(\"Month\")\n",
    "        ax.set_ylabel(\"Number of Reviews\")\n",
    "        ax.set_xticklabels(pivot.index, rotation=45)\n",
    "        ax.grid(True)\n",
    "        ax.legend([\"Positive\", \"Negative\", \"Neutral\"])\n",
    "        st.pyplot(fig)\n",
    "\n",
    "    # ==============================\n",
    "    # 5. Verified vs Unverified\n",
    "    # ==============================\n",
    "    elif chart_option == \"Verified vs Non-Verified\":\n",
    "        fig, ax = plt.subplots(figsize=(4, 3))\n",
    "        sns.barplot(\n",
    "            ax=ax,\n",
    "            x='verified_purchase',\n",
    "            hue='verified_purchase',\n",
    "            y='rating',\n",
    "            data=df,\n",
    "            palette='coolwarm',\n",
    "            estimator='mean',\n",
    "            errorbar=None\n",
    "        )\n",
    "        ax.set_title(\"Average Rating: Verified vs Non-Verified Users\", fontsize=14, weight='bold')\n",
    "        ax.set_xlabel(\"Verified Purchase Status\")\n",
    "        ax.set_ylabel(\"Average Rating\")\n",
    "        ax.set_ylim(0, 5)\n",
    "        ax.grid(axis='y', linestyle='--', alpha=0.4)\n",
    "        st.pyplot(fig)\n",
    "\n",
    "    # ==============================\n",
    "    # 6. Review Length vs Sentiment\n",
    "    # ==============================\n",
    "    elif chart_option == \"Review length vs Sentiment\":\n",
    "        fig, ax = plt.subplots(figsize=(3, 2))\n",
    "        sns.boxplot(x=\"sentiment\", y=\"review_length\", data=df, palette=\"viridis\", ax=ax)\n",
    "        ax.set_title(\"Review Length vs Sentiment\")\n",
    "        st.pyplot(fig)\n",
    "\n",
    "    # ==============================\n",
    "    # 7. Sentiment by Location\n",
    "    # ==============================\n",
    "    elif chart_option == \"Sentiment by Location\":\n",
    "        loc_sentiment = df.groupby(\"location\")[\"rating\"].mean().sort_values(ascending=False).head(10)\n",
    "        fig, ax = plt.subplots(figsize=(4, 2))\n",
    "        loc_sentiment.plot(kind=\"bar\", ax=ax, color=\"skyblue\")\n",
    "        ax.set_title(\"Top 10 Locations by Sentiment\")\n",
    "        st.pyplot(fig)\n",
    "\n",
    "    # ==============================\n",
    "    # 8. Platform Sentiment\n",
    "    # ==============================\n",
    "    elif chart_option == \"Platform Sentiment\":\n",
    "        fig, ax = plt.subplots(figsize=(4, 2))\n",
    "        sns.barplot(x=\"platform\", y=\"rating\", data=df, estimator=\"mean\", palette=\"cubehelix\", ax=ax)\n",
    "        ax.set_title(\"Platform-wise Sentiment\")\n",
    "        st.pyplot(fig)\n",
    "\n",
    "    # ==============================\n",
    "    # 9. ChatGPT Version vs Sentiment\n",
    "    # ==============================\n",
    "    elif chart_option == \"Sentiment by ChatGptVersion\":\n",
    "        version_sentiment = df.groupby([\"version\", \"sentiment\"]).size().reset_index(name=\"count\")\n",
    "        fig, ax = plt.subplots(figsize=(4, 2))\n",
    "        sns.barplot(\n",
    "            data=version_sentiment,\n",
    "            x=\"version\",\n",
    "            y=\"count\",\n",
    "            hue=\"sentiment\",\n",
    "            palette={\"Positive\": \"green\", \"Negative\": \"red\", \"Neutral\": \"blue\"},\n",
    "            ax=ax\n",
    "        )\n",
    "        ax.set_title(\"Sentiment Distribution by ChatGPT Version\", fontsize=12, weight=\"bold\")\n",
    "        ax.set_xlabel(\"ChatGPT Version\")\n",
    "        ax.set_ylabel(\"Number of Reviews\")\n",
    "        plt.xticks(rotation=45)\n",
    "        ax.legend(title=\"Sentiment\")\n",
    "        st.pyplot(fig)\n",
    "\n",
    "    # ==============================\n",
    "    # 10. Negative Feedback Themes\n",
    "    # ==============================\n",
    "    elif chart_option == \"Topic Modeling on Negative Reviews\":\n",
    "        negative_text = \" \".join(df[df[\"sentiment\"] == \"Negative\"][\"cleaned_text\"])\n",
    "        wc = WordCloud(width=600, height=300, background_color=\"white\", colormap=\"Reds\").generate(negative_text)\n",
    "        fig, ax = plt.subplots(figsize=(3, 2))\n",
    "        ax.imshow(wc, interpolation=\"bilinear\")\n",
    "        ax.axis(\"off\")\n",
    "        st.pyplot(fig)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LQEEC10kx1sL",
    "outputId": "1f1939ea-a767-483e-9a5e-eee47a597008"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Streamlit app is live at: NgrokTunnel: \"https://10dd11440e5c.ngrok-free.app\" -> \"http://localhost:8501\"\n"
     ]
    }
   ],
   "source": [
    "from pyngrok import ngrok\n",
    "import time\n",
    "\n",
    "# Kill any previous tunnels\n",
    "ngrok.kill()\n",
    "\n",
    "# Set auth token (optional for free use)\n",
    "ngrok.set_auth_token(\"31AnkqDpmepL3vshXWcvaULCpEc_3R66vAgymti8ggGU2mp83\")\n",
    "\n",
    "# Start Streamlit in background\n",
    "!streamlit run /content/trans_streamlitapp1.py &>/content/log.txt &\n",
    "\n",
    "# Wait for Streamlit to start\n",
    "time.sleep(5)  # You can increase to 10 if needed\n",
    "\n",
    "# Create ngrok tunnel\n",
    "public_url = ngrok.connect(addr=\"8501\", proto=\"http\")\n",
    "print(\"✅ Streamlit app is live at:\", public_url)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
